# üé• Inference API

O projeto Inference API foi desenvolvido como parte das sprints 4 e 5 do programa de bolsas Compass UOL para forma√ß√£o em machine learning na AWS. Ele consiste em uma API que realiza infer√™ncia com um modelo de machine learning treinado com o framework XGBoost no Amazon SageMaker.

## üìñ √çndice

- [üìù Descri√ß√£o do projeto](#-inference-api)
- [‚öôÔ∏è Tecnologias utilizadas](#Ô∏è-tecnologias-utilizadas)
- [üèõÔ∏è Arquitetura](#Ô∏è-arquitetura)
- [üîé Sobre o Modelo](#-sobre-o-modelo)
- [üöÄ Execu√ß√£o e utiliza√ß√£o](#-execu√ß√£o-e-utiliza√ß√£o)
  - [Pr√©-requisitos](#pr√©-requisitos)
  - [Passos para execu√ß√£o local dos notebooks com o Jupyter Notebooks](#passos-para-execu√ß√£o-local-dos-notebooks-com-o-jupyter-notebooks)
  - [Passos para execu√ß√£o local da API com Python e Docker](#passos-para-execu√ß√£o-local-da-api-com-python-e-docker)
  - [Passos para o deploy com o EC2 e Docker](#passos-para-o-deploy-com-o-ec2-e-docker)
- [üß± Estrutura do projeto](#-estrutura-do-projeto)
- [üöß Desafios e solu√ß√µes](#-desafios-e-solu√ß√µes)
- [üë• Contribuidores](#-contribuidores)

## ‚öôÔ∏è Tecnologias Utilizadas

- **Python**
  - **Boto3** para possibilitar a integra√ß√£o com os servi√ßos utilizados da AWS
  - **FastAPI** para cria√ß√£o de uma API com endpoint de consulta ao modelo
  - **Pandas, Numpy, Matplotlib, Sklearn, Seaborn** para a an√°lise, tratamento, previs√£o e avalia√ß√£o dos dados
- **AWS**
  - **SageMaker** para o treinamento do modelo de aprendizagem
  - **RDS** para trabalhar as bases de dados
  - **EC2** para deploy da aplica√ß√£o
  - **S3** para armazenar o modelo de aprendizado
- **Docker e Docker Compose** para criar imagem do projeto e execut√°-lo localmente por linha de comando e arquivo de configura√ß√£o
- **Jupyter Notebooks** para a cria√ß√£o dos notebooks de ci√™ncia de dados
- **Git/Github** para o controle de vers√£o do c√≥digo
- **Trello** para o gerenciamento e organiza√ß√£o das tarefas do projeto

## üèõÔ∏è Arquitetura

![Esquema arquitetura](assets/sprint4-5.jpg)

- **Code:** Implementa√ß√£o do c√≥digo pela equipe de programadores.
- **RDS:** Armazenamento do dataset original e processado.
- **SageMaker:** Treinamento do modelo.
- **S3:** Armazenamento do modelo e dos dados de treinamento e teste.
- **EC2:** Execu√ß√£o do container docker com a API de infer√™ncia com o modelo em ambiente da nuvem para expor aos usu√°rios.

## üîé Sobre o Modelo

- Das 19 colunas do dataset, foram escolhidas 11 colunas, e as demais foram removidas baseado na explora√ß√£o dos dados, onde foram avaliados as estatisticas, frequ√™ncia, correla√ß√£o e vari√¢ncia, e ao final foi obtido o resultado de que al√©m da coluna ``avg_price_per_room``, era necess√°rio remover outras colunas para se obter um melhor resultado no treinamento do modelo, que foram esssas: ``Booking_ID``, ``booking_status``, ``no_of_weekend_nights``, ``no_of_week_nights``, ``repeated_guest``, ``no_of_previous_cancellations``, ``no_of_previous_bookings_not_canceled``.

- Durante o treinamento, o framework escolhido para classifica√ßa√µ foi o ``XGBoost`` por conta da sua efici√™ncia em trabalhar com grandes bases de dados, a op√ß√£o que ele disponibiliza para treinamento de classifica√ß√£o, o que √© necess√°rio neste projeto, resultados comprovados e a efici√™ncia final no modelo treinado.

- Foram necess√°rios definir alguns hiperpar√¢metros para o treinamento para obter melhor efici√™ncia e resultado, dentre eles o principal foi o objective ``multi:softmax`` e o num_class como ``3`` para poder trabalhar com problemas de classifica√ß√£o de 3 classes que √© o objetivo alvo do projeto, e al√©m dele, foi definido tamb√©m o eval_metric como ``mlogloss``, alpha como ``0.9975...``, eta como ``0.3825...``, min_child_weight como ``3.9469`` e num_round como ``259``.

- Ao final, o modelo obteve uma acur√°cia de ``0.8493``, o que foi considerada como boa.

## üöÄ Execu√ß√£o e Utiliza√ß√£o

### Pr√©-requisitos

- **Git** *(clonar o reposit√≥rio e visualizar o versionamento)*
- **Python** *(baixar as depend√™ncias necess√°rias e executar a api e os notebooks localmente)*
- **Docker** *(criar imagem do projeto e execut√°-lo localmente por linha de comando)*
- **Docker Compose** *(criar imagem do projeto e executar localmente por arquivo de configura√ß√£o)*
- **Conta na AWS** *(fazer deploy da api no EC2, treinar o modelo no SageMaker, armazenar dataset no RDS, armazenar modelo no S3)*
- **AWS CLI** *(executar comandos via terminal para interagir com os servi√ßos AWS igual o console da AWS)*

---

### Passos para execu√ß√£o local dos notebooks com o Jupyter Notebooks

Este guia fornece instru√ß√µes passo a passo para configurar e fazer a execu√ß√£o dos notebooks de machine learning usando o jupyter notebooks para criar o modelo necess√°rio para realizar infer√™ncia na API.

**Requisitos: Conta AWS com uma VPC default ou outra configurada corretamente, AWS CLI instalada e configurada com as credenciais de acesso de forma default ou com profiles.**

1. Os Notebooks pode ser executado em outros locais diferentes do Jupyter Notebooks, assim como o Jupyter Notebooks tamb√©m pode ser executado em v√°rios locais. E neste passo a passo, ele ser√° executado no VSCode, aqui est√° a documenta√ß√£o de [download e configura√ß√£o do VSCode para python](https://code.visualstudio.com/docs/python/python-quick-start).

2. Ap√≥s baixar e instalar o VSCode, ser√° necess√°rio instalar e configurar o ambiente do Jupyter notebooks, aqui est√° a documenta√ß√£o de [configurar o VSCode para o Jupyter Notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).

3. Clone e abra projeto no VSCode:

    ```bash
   git clone -b grupo-6 --single-branch https://github.com/Compass-pb-aws-2024-MAIO-A/sprints-4-5-pb-aws-maio.git
   cd sprints-4-5-pb-aws-maio
   code .
   ```

4. Instale as depend√™ncias:

   ```bash
   pip install -r requirements.txt
   ```

5. Configure as vari√°veis de ambiente:

   Crie um arquivo `.env` na raiz do projeto seguindo o modelo do arquivo de exemplo **[.env.example](.env.example)** do reposit√≥rio.

6. Certifique-se de que sua AWS CLI est√° logada corretamente e com as permiss√µes necess√°rias para poder executar os notebooks localizados na pasta [ml-lab/](ml-lab).

7. Executar os notebooks seguindo a ordem dos arquivos do 0 ao 4, onde ser√° explicado o que cada notebook far√°:

    - **00-storage-original-dataset**: Ir√° ler o dataset.csv, criar uma inst√¢ncia no RDS e depois armazenar o dataset sem modifica√ß√µes no RDS para futuras consultas.

    - **01-exploratory-data-analysis**: Ir√° ler o dataset armazenado no RDS e fazer a an√°lise dos dados para que os processamentos possam ser feitos.

    - **02-data-processing-and-storage**: Ir√° fazer os devidos processamentos dos dados analisados e depois inser√≠-los no RDS em outra tabela.

    - **03-training-xgboost-model**: Ir√° utilizar os dados processados para o treinamento e teste do modelo com o framework Xgboost no SageMaker, onde ser√° criado um Bucket S3 e uma Role IAM com as permiss√µes requisitadas pelo SageMaker, e os dados de treinamento e teste ser√£o armazenados no S3 junto do modelo que ser√° treinado com esses mesmos dados.

    - **04-prediction-and-evaluation**: Ir√° fazer o download e descompacta√ß√£o do modelo treinado que foi armazenado no S3, e logo ap√≥s ser√£o feitas as previs√µes e avalia√ß√µes do modelo com a lib sklearn.

    **OBS:** O notebook `00-storage-original-dataset` tem um aviso sobre a parte `02 - Create RDS instance if not exists`, nesta parte tem que esperar a inst√¢ncia do RDS ser processada e ficar em execu√ß√£o para que o endpoint possa ser consultado e utilizado, isso pode ser visto no console AWS, s√£o aproximadamente *5min*.

---

### Passos para execu√ß√£o local da API com Python e Docker

Este guia fornece instru√ß√µes passo a passo para configurar e fazer a execu√ß√£o da API de infer√™ncia no ambiente local ap√≥s a cria√ß√£o do modelo usando os notebooks deste projeto seguindo os passos acima.

1. **Prepara√ß√£o do modelo para infer√™ncia**:

    Executar o script **[prepare_model.py](scripts/prepare_model.py)** que ir√° buscar no bucket S3 definido nas vari√°veis de ambiente, o √∫ltimo modelo que foi treinado usando os notebooks desse projeto, e ir√° baix√°-lo e descompact√°-lo para que seja utilizado na infer√™ncia. Para executar o script, use:

    ```bash
    python scripts/prepare_model.py
    ```

2.
   1. **Execu√ß√£o local com Python**:

      ```bash
      python api/main.py
      ```

   2. **Build e execu√ß√£o local com Docker Compose**:

      ```bash
      docker compose up -d
      ```

3. **Acesse a aplica√ß√£o**:

   Abra o navegador e v√° para `http://localhost:8000/docs` nas execu√ß√µes via Python ou `http://localhost/docs` nas execu√ß√µes via Docker para acessar a interface do **Swagger** onde haver√° um modelo com instru√ß√µes para utilizar a API.

---

### Passos para o deploy com o EC2 e Docker

Este guia fornece instru√ß√µes para configurar e fazer o deploy da imagem Docker da API de infer√™ncia no AWS EC2 via AWS CLI, e que tamb√©m pode ser feito via console da AWS.

Para mais informa√ß√µes sobre os comandos e flags fornecidas no passo a passo, al√©m de outros comandos, confira a [comandos AWS CLI](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/index.html).

1. Na raiz do projeto, fazer o build da imagem docker da aplica√ß√£o e post√°-la no docker hub.

    ```bash
    docker build -t docker-hub-user/inference-image:latest . --no-cache
    docker push docker-hub-user/inference-image:latest

    # OBS: substitua "docker-hub-user" pelo seu nome de usu√°rio do DockerHub.
    ```

2. Editar o script `ec2.sh` localizado em [scripts/ec2.sh](scripts/ec2.sh) para alterar a imagem de cria√ß√£o do container baseado reposit√≥rio docker hub, ex: `fulano/inference-image:latest`.

3. Configurar um Security Group com a porta `HTTP 80` liberada para `0.0.0.0/0` na VPC padr√£o ou em outra.

4. Executar uma inst√¢ncia EC2 com associa√ß√£o de um ip p√∫blico passando o script `ec2.sh` editado com os valores corretos e definindo a VPC e o Security Group expl√≠citos caso n√£o sejam os default.

   ```bash
   aws ec2 run-instances \
   --image-id ami-08a0d1e16fc3f61ea \
   --count 1 \
   --instance-type t2.micro \
   --associate-public-ip-address \
   --user-data file://scripts/ec2.sh \
   --tag-specifications ResourceType=instance,Tags='[{Key=Project,Value=project},{Key=CostCenter,Value=costcenter},{Key=Name,Value=movies-mania}]' \
   ResourceType=volume,Tags='[{Key=Project,Value=project},{Key=CostCenter,Value=costcenter},{Key=Name,Value=movies-mania}]' \
   --profile profile_name

   # a flag --profile aqui s√≥ √© necess√°ria caso o perfil usado n√£o seja o default
   # neste caso, substitua "profile-name" pelo nome do seu profile
   ```

5. Aguardar a cria√ß√£o da inst√¢ncia e configura√ß√£o do container, e depois de alguns minutos acessar o ip p√∫blico da inst√¢ncia que pode ser visto acessando a inst√¢ncia em execu√ß√£o na p√°gina das inst√¢ncias EC2 no console AWS.

## üß± Estrutura do projeto

```plaintext
.
‚îú‚îÄ‚îÄ .venv/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ assets/
‚îú‚îÄ‚îÄ ml-lab/
‚îÇ   ‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îú‚îÄ‚îÄ env/
‚îÇ   ‚îú‚îÄ‚îÄ iam/
‚îÇ   ‚îú‚îÄ‚îÄ rds/
‚îÇ   ‚îú‚îÄ‚îÄ s3/
‚îÇ   ‚îú‚îÄ‚îÄ notebooks.ipynb
‚îú‚îÄ‚îÄ scripts/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ dockerfile
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
```

---

- **.venv/** - Cont√©m as depend√™ncia da aplica√ß√£o e dos notebooks.
- **api/** - Cont√©m as rotas, esquemas e utilit√°rios da API.
  - **routes/** - Cont√©m as rotas da API.
  - **utils/** - Cont√©m os esquemas da API.
  - **schemas/** - Cont√©m os utilit√°rios da API.
  - **main.py** - Arquivo principal para execu√ß√£ da API.
- **assets/** - Cont√©m os diagramas dos esquemas de arquitetura e do dataset.
- **ml-lab/** - Cont√©m os notebooks respons√°veis pela an√°lise e tratamento de dados al√©m da cria√ß√£o do modelo.
  - **dataset/** - Cont√©m o dataset base.
  - **env/** - Cont√©m vari√°veis de ambiente relacionadas a configura√ß√£o dos servi√ßos AWS.
  - **iam/** - Cont√©m scripts para a cria√ß√£o, anexa√ß√£o e recupera√ß√£o de roles IAM.
  - **rds/** - Cont√©m scripts para a cria√ß√£o da engine do rds bem como a cri√ß√£o e obten√ß√£o de inst√¢ncias.
  - **s3/** - Cont√©m scripts para a cria√ß√£o de um bucket S3 se ele n√£o existir.
- **scripts/** - Cont√©m scipts utilit√°rios para a execu√ß√£o do docker no EC2 e a prepara√ß√£o do modelo para uso.
- **.env** - Arquivo de configura√ß√£o das vari√°veis de ambiente.
- **.env.example** - Exemplo do arquivo `.env` com as vari√°veis de ambiente necess√°rias.
- **.gitignore** - Arquivo de configura√ß√£o para ignorar arquivos no reposit√≥rio Git.
- **docker-compose.yml** - Arquivo de configura√ß√£o para constru√ß√£o e execu√ß√£o do projeto com Docker Compose.
- **Dockerfile** - Arquivo de configura√ß√£o para a constru√ß√£o da imagem Docker.
- **README.md** - Documenta√ß√£o do projeto.
- **requirements.txt** - Arquivo contendo quais depend√™ncias s√£o instaladas no projeto bem como suas respectivas vers√µes.
- **xgboost-model** - Arquivo bin com o modelo treinado para fazer infer√™ncia na aplica√ß√£o, ele √© baixado quando se executa o script prepare_model.py ap√≥s a cria√ß√£o do modelo no SageMaker.

## üöß Desafios e Solu√ß√µes

### Compreen√ß√£o do problema

Levamos um tempo para compreender a proposta da sprint, revisitando algumas vezes o readme e tirando algumas d√∫vidas nas dailys conseguimos compreender melhor

### Concep√ß√£o da Solu√ß√£o

Foi bastante demorado a concep√ß√£o da solu√ß√£o pois n√£o soubemos imediatamente como solucionar√≠amos o problema, como por exemplo que algoritmo de aprendizado usar√≠amos ou como processar√≠amos e analisar√≠amos os dados.

### Integra√ß√£o com os Servi√ßos AWS em Ambiente Local

Era requisito usarmos o sagemaker localmente para evitar custos extras, sendo assim tivemos que fazer um esfor√ßo extra para descobrir como integrar os servi√ßos aws ao nosso ambiente local

## Infer√™ncia

Enfretamos um problema com a infer√™ncia onde de acordo com a ordem dos dados inseridos o resultado mudava drasticamente, ent√£o foi necess√°rio ordernar as colunas durante o treinamento e durante a infer√™ncia

## üë• Contribuidores

- **[Jo√£o Guilherme](https://github.com/Joao-Patriota)**
- **[Richard Freitas](https://github.com/wesleyfreit)**
- **[Ytallo Alves](https://github.com/YtalloAlves)**
